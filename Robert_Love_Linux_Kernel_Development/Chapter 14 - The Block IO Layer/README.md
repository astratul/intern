# The Block I/O Layer
Block devices are hardware devices distinguished by the random (that is, not necessarily sequential) access of fixed-size chunks of data.The fixed-size chunks of data are called blocks.The most common block device is a hard disk, but many other block devices exist, such as floppy drives, Blu-ray readers, and flash memory. Notice how these are all devices on which you mount a filesystem—filesystems are the lingua franca of block devices.
The other basic type of device is a character device. Character devices, or char devices, are accessed as a stream of sequential data, one byte after another.Example character devices are serial ports and keyboards. If the hardware device is accessed as a stream of data, it is implemented as a character device. On the other hand, if the device is accessed randomly (nonsequentially), it is a block device. The difference comes down to whether the device accesses data randomly—in other words, whether the device can seek to one position from another.As an example, consider the keyboard.As a driver, the keyboard provides a stream of data.

The hard disk’s data is accessed randomly, and not as a stream; therefore, the hard disk is a block device. Managing block devices in the kernel requires more care, preparation, and work than managing character devices. Character devices have only one position—the current one—whereas block devices must be able to navigate back and forth between any location on the media. Indeed, the kernel does not have to provide an entire subsystem dedicated to the management of character devices, but block devices receive exactly that. Such a subsystem is a necessity partly because of the complexity of block devices.

## Anatomy of a Block Device

The smallest addressable unit on a block device is a sector. Sectors come in various powers of two, but 512 bytes is the most common size.The sector size is a physical property of the device, and the sector is the fundamental unit of all block devices—the device cannot address or operate on a unit smaller than the sector, although many block devices can operate on multiple sectors at one time.
Software has different goals and therefore imposes its own smallest logically addressable unit, which is the block.The block is an abstraction of the filesystem—filesystems can be accessed only in multiples of a block.Although the physical device is addressable at the sector level, the kernel performs all disk operations in terms of blocks. Because the device’s smallest addressable unit is the sector, the block size can be no smaller than the sector and must be a multiple of a sector. Furthermore, the kernel (as with hardware and the sector) needs the block to be a power of two.The kernel also requires that a block be no larger than the page size. Therefore, block sizes are a power-of-two multiple of the sector size and are not greater than the page size. Common block sizes are 512 bytes, 1 kilobyte, and 4 kilobytes.
Somewhat confusingly, some people refer to sectors and blocks with different names. Sectors, the smallest addressable unit to the device, are sometimes called “hard sectors” or “device blocks.” Meanwhile, blocks, the smallest addressable unit to the filesystem, are sometimes referred to as “filesystem blocks” or “I/O blocks.”

## Buffers and Buffer Heads
When a block is stored in memory—say, after a read or pending a write—it is stored in a buffer. Each buffer is associated with exactly one block.The buffer serves as the object that represents a disk block in memory. Recall that a block is composed of one or more sectors but is no more than a page in size.Therefore, a single page can hold one or more blocks in memory.
Each buffer is associated with a descriptor.The descriptor is called a buffer head and is of type struct buffer_head .The buffer_head structure holds all the information that the kernel needs to manipulate buffers and is defined in <linux/buffer_head.h> .
The legal flags are stored in the bh_state_bits enumeration, which is defined in <linux/buffer_head.h> .
The bh_state_bits enumeration also contains as the last value in the list a BH_PrivateStart flag.This is not a valid state flag but instead corresponds to the first usable bit of which other code can make use.All bit values equal to and greater than BH_PrivateStart are not used by the block I/O layer proper, so these bits are safe to use by individual drivers who want to store information in the b_state field. Drivers can base the bit values of their internal flags off this flag and rest assured that they are not encroaching on an official bit used by the block I/O layer.
The second issue with buffer heads is that they describe only a single buffer.When used as the container for all I/O operations, the buffer head forces the kernel to break up potentially large block I/O operations (say, a write) into multiple buffer_head structures. This results in needless overhead and space consumption.


## The bio Structure
The basic container for block I/O within the kernel is the bio structure, which is defined in <linux/bio.h> .This structure represents block I/O operations that are in flight (active) as a list of segments.A segment is a chunk of a buffer that is contiguous in memory.Thus, individual buffers need not be contiguous in memory. By allowing the buffers to be described in chunks, the bio structure provides the capability for the kernel to perform block I/O operations of even a single buffer from multiple locations in memory. Vector I/O such as this is called scatter-gather I/O.
The primary purpose of a bio structure is to represent an in-flight block I/O operation.To this end, the majority of the fields in the structure are housekeeping related. The most important fields are bi_io_vec , bi_vcnt , and bi_idx .

### I/O vectors

The bi_io_vec field points to an array of bio_vec structures.These structures are used as lists of individual segments in this specific block I/O operation. Each bio_vec is treated as a vector of the form <page, offset, len> , which describes a specific segment: the physical page on which it lies, the location of the block as an offset into the page, and the length of the block starting from the given offset.The full array of these vectors describes the entire buffer.The bio_vec structure is defined in <linux/bio.h>
In each given block I/O operation, there are bi_vcnt vectors in the bio_vec array starting with bi_io_vec .As the block I/O operation is carried out, the bi_idx field is used to point to the current index into the array.
In summary, each block I/O request is represented by a bio structure. Each request is composed of one or more blocks, which are stored in an array of bio_vec structures.
The bi_idx field is used to point to the current bio_vec in the list, which helps the block I/O layer keep track of partially completed block I/O operations.A more important usage, however, is to allow the splitting of bio structures.With this feature, drivers implementing a Redundant Array of Inexpensive Disks (RAID, a hard disk setup that enables single volumes to span multiple disks for performance and reliability purposes) can take a single bio structure, initially intended for a single device and split it among the multiple hard drives in the RAID array.All the RAID driver needs to do is copy the bio structure and update the bi_idx field to point to where the individual drive should start its operation.
The bio structure maintains a usage count in the bi_cnt field.When this field reaches zero, the structure is destroyed and the backing memory is freed.The following two functions manage the usage counters for you.
```
void bio_get(struct bio *bio)
void bio_put(struct bio *bio)
```
The former increments the usage count, whereas the latter decrements the usage count (and, if the count reaches zero, destroys the bio structure). Before manipulating an in-flight bio structure, be sure to increment its usage count to make sure it does not complete and deallocate out from under you.When you finish, decrement the usage count in turn.
Finally, the bi_private field is a private field for the owner (that is, creator) of the structure.As a rule, you can read or write this field only if you allocated the bio structure.

### The Old Versus the New
The difference between buffer heads and the new bio structure is important.The bio structure represents an I/O operation, which may include one or more pages in memory. On the other hand, the buffer_head structure represents a single buffer, which describes a single block on the disk. Because buffer heads are tied to a single disk block in a single page, buffer heads result in the unnecessary dividing of requests into block-sized chunks, only to later reassemble them. Because the bio structure is lightweight, it can describe discontiguous blocks and does not unnecessarily split I/O operations. 

Switching from struct buffer_head to struct bio provided other benefits, as well:
* The bio structure can easily represent high memory, because struct bio deals with only physical pages and not direct pointers.
* The bio structure can represent both normal page I/O and direct I/O (I/O operations that do not go through the page cache 
* The bio structure makes it easy to perform scatter-gather (vectored) block I/O operations, with the data involved in the operation originating from multiple physical pages.
* The bio structure is much more lightweight than a buffer head because it contains only the minimum information needed to represent a block I/O operation and not unnecessary information related to the buffer itself.
The concept of buffer heads is still required, however; buffer heads function as descriptors, mapping disk blocks to pages.The bio structure does not contain any information about the state of a buffer—it is simply an array of vectors describing one or more segments of data for a single block I/O operation, plus related information. In the current setup, the buffer_head structure is still needed to contain information about buffers while the bio structure describes in-flight I/O. Keeping the two structures separate enables each to remain as small as possible.

## Request Queues
Block devices maintain request queues to store their pending block I/O requests.The request queue is represented by the request_queue structure and is defined in <linux/blkdev.h> .The request queue contains a doubly linked list of requests and associated control information. Requests are added to the queue by higher-level code in the kernel, such as filesystems.As long as the request queue is nonempty, the block device driver associated with the queue grabs the request from the head of the queue and submits it to its associated block device. Each item in the queue’s request list is a single request, of type struct request .
Individual requests on the queue are represented by struct request , which is also defined in <linux/blkdev.h> . Each request can be composed of more than one bio structure because individual requests can operate on multiple consecutive disk blocks.

## I/O Schedulers
Simply sending out requests to the block devices in the order that the kernel issues them, as soon as it issues them, results in poor performance. One of the slowest operations in a modern computer is disk seeks. Each seek—positioning the hard disk’s head at the location of a specific block—takes many milliseconds. Minimizing seeks is absolutely crucial to the system’s performance.
Therefore, the kernel does not issue block I/O requests to the disk in the order they are received or as soon as they are received. Instead, it performs operations called merging and sorting to greatly improve the performance of the system as a whole. The subsystem of the kernel that performs these operations is called the I/O scheduler.
The I/O scheduler divides the resource of disk I/O among the pending block I/O requests in the system. It does this through the merging and sorting of pending requests in the request queue.The I/O scheduler is not to be confused with the process scheduler , which divides the resource of the processor among the processes on the system.The two subsystems are similar in nature but not the same.
Both the process scheduler and the I/O scheduler virtualize a resource among multiple objects. In the case of the process scheduler, the processor is virtualized and shared among the processes on the system.This provides the illusion of virtualization inherent in a muLtitasking and timesharing operating system, such as any Unix. On the other hand, the I/O scheduler virtualizes block devices among multiple outstanding block requests.This is done to minimize disk seeks and ensure optimum disk performance.

### The Job of an I/O Scheduler

An I/O scheduler works by managing a block device’s request queue. It decides the order of requests in the queue and at what time each request is dispatched to the block device. It manages the request queue with the goal of reducing seeks, which results in greater global throughput.The modifier “global” here is important.An I/O scheduler, very openly, is unfair to some requests at the expense of improving the overall performance of the system.
I/O schedulers perform two primary actions to minimize seeks: merging and sorting. Merging is the coalescing of two or more requests into one. By merging requests, the I/O scheduler reduces the overhead of multiple requests down to a single request. More important only a single command needs to be issued to the disk and servicing the multiple requests can be done without seeking. Consequently, merging requests reduces overhead and minimizes seeks.
The entire request queue is kept sorted, sectorwise, so that all seeking activity along the queue moves (as much as possible) sequentially over the sectors of the hard disk.The goal is not just to minimize each individual seek but to minimize all seeking by keeping the disk head moving in a straight line.Because of this similarity, I/O schedulers (or sometimes just their sorting algorithm) are called elevators.

### The Linus Elevator
The first I/O scheduler is called the Linus Elevator. The Linus Elevator performs both merging and sorting.When a request is added to the queue, it is first checked against every other pending request to see whether it is a possible candidate for merging.The Linus Elevator I/O scheduler performs both front and back merging.The type of merging performed depends on the location of the existing adjacent request. If the new request immediately proceeds an existing request, it is front merged. Conversely, if the new request immediately precedes an existing request, it is back merged. Because of the way files are laid out (usually by increasing sector number) and the I/O operations performed in a typical workload (data is normally read from start to finish and not in reverse), front merging is rare compared to back merging. Nonetheless, the Linus Elevator checks for and performs both types of merge. If the merge attempt fails, a possible insertion point in the queue (a location in the queue where the new request fits sectorwise between the existing requests) is then sought. If one is found, the new request is inserted there. If a suitable location is not found, the request is added to the tail of the queue.
The Linus elevator is implemented in block/elevator.c .

### The Deadline I/O Scheduler

The Deadline I/O scheduler sought to prevent the starvation caused by the Linus Elevator. In the interest of minimizing seeks, heavy disk I/O operations to one area of the disk can indefinitely starve request operations to another part of the disk. Indeed, a stream of requests to the same area of the disk can result in other far-off requests never being serviced.This starvation is unfair.Deadline I/O scheduler implements several features to ensure that request starvation in general, and read starvation in specific, is minimized.
The Deadline I/O scheduler lives in block/deadline-iosched.c .
### The Anticipatory I/O Scheduler
Although the Deadline I/O scheduler does a great job minimizing read latency, it does so at the expense of global throughput. Consider a system undergoing heavy write activity. Every time a read request is submitted, the I/O scheduler quickly rushes to handle the read request.This results in the disk seeking over to where the read is, performing the read operation, and then seeking back to continue the ongoing write operation, repeating this little charade for each read request.The preference toward read requests is a good thing, but the resulting pair of seeks (one to the location of the read request and another back to the ongoing write) is detrimental to global disk throughput.The Anticipatory I/O scheduler aims to continue to provide excellent read latency, but also provide excellent global throughput.
First, the Anticipatory I/O scheduler starts with the Deadline I/O scheduler as its base. Therefore, it is not entirely different.The Anticipatory I/O scheduler implements three queues (plus the dispatch queue) and expirations for each request, just like the Deadline I/O scheduler.The major change is the addition of an anticipation heuristic.

The Anticipatory I/O scheduler attempts to minimize the seek storm that accompanies read requests issued during other disk I/O activity.

The Anticipatory I/O scheduler lives in the file block/as-iosched.c in the kernel source tree. It performs well across most workloads. It is ideal for servers, although it performs poorly on certain uncommon but critical workloads involving seek-happy databases.
### The Complete Fair Queuing I/O Scheduler
The Complete Fair Queuing (CFQ) I/O scheduler is an I/O scheduler designed for specialized workloads, but that in practice actually provides good performance across multiple workloads. It is fundamentally different from the previous I/O schedulers that have been covered, however.
The CFQ I/O scheduler assigns incoming I/O requests to specific queues based on the process originating the I/O request. For example, I/O requests from process foo go in foo’s queues, and I/O requests from process bar go in bar’s queue.Within each queue, requests are coalesced with adjacent requests and insertion sorted.The queues are thus kept sorted sectorwise, as with the other I/O scheduler’s queues.The difference with the CFQ I/O scheduler is that there is one queue for each process submitting I/O.

The Complete Fair Queuing I/O scheduler lives in block/cfq-iosched.c . It is recommended for desktop workloads, although it performs reasonably well in nearly all workloads without any pathological corner cases. It is now the default I/O scheduler in Linux.
### The Noop I/O Scheduler
A fourth and final I/O scheduler is the Noop I/O scheduler, so named because it is basically a noop—it does not do much.The Noop I/O scheduler does not perform sorting or any other form of seek-prevention whatsoever. In turn, it has no need to implement anything akin to the slick algorithms to minimize request latency that you saw in the previous three I/O schedulers.
The Noop I/O scheduler does perform merging, however, as its lone chore.When a new request is submitted to the queue, it is coalesced with any adjacent requests.

Noop I/O Scheduler truly is a noop, merely maintaining the request queue in near-FIFO order, from which the block device driver can pluck requests.
The Noop I/O scheduler’s lack of hard work is with reason. It is intended for block devices that are truly random-access, such as flash memory cards. If a block device has little or no overhead associated with “seeking,” then there is no need for insertion sorting of incoming requests, and the Noop I/O scheduler is the ideal candidate.
The Noop I/O scheduler lives in block/noop-iosched.c . It is intended only for random-access devices.
### I/O Scheduler Selection
By default, block devices use the Complete Fair Queuing I/O scheduler.This can be overridden via the boot-time option elevator=foo on the kernel command line, where foo is a valid and enabled I/O Scheduler.
##### Parameters Given to elevator Option
Parameter - I/O Scheduler
* as - Anticipatory
* cfq - Complete Fair Queuing
* deadline - Deadline
* noop - Noop
For example, the kernel command line option elevator=as would enable use of the Anticipatory I/O scheduler for all block devices, overriding the default Complete Fair Queuing scheduler.